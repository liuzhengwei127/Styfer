{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VGG19 model\n",
    "First, we'd better to load the pretrained VGG19 model from ``torchvision.models``, which saves us lots of time of training the VGG19 from scratch.\n",
    "\n",
    "VGG19 is split into two parts: \n",
    "* ``vgg19.features``, which are the convolutional and pooling layers\n",
    "* ``vgg19.classifier``, which are the three linear and classifier layers\n",
    "\n",
    "We just need to load the ``features`` portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load features of vgg19 from torchvision models\n",
    "vgg19_model = models.vgg19(pretrained = True).features\n",
    "\n",
    "for param in vgg19_model.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# move vgg19 model to GPU if it is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg19_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Transform Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, shape=None, max_size=800):\n",
    "    '''\n",
    "    Load in and transform the target image defined by img_path\n",
    "    '''\n",
    "    \n",
    "    image = Image.open(img_path)\n",
    "    ### .convert('RGB')?\n",
    "    \n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "    else:\n",
    "        if max(image.size) > max_size:\n",
    "            size = max_size\n",
    "        else:\n",
    "            size = max(image.size)\n",
    "    \n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    image = img_transform(image)[:3,:,:]\n",
    "    print(size)\n",
    "    print(image.shape)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 400)\n",
      "torch.Size([3, 400, 400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7294, 0.7255, 0.7216,  ..., 0.4824, 0.4902, 0.4941],\n",
       "         [0.7294, 0.7255, 0.7216,  ..., 0.4824, 0.4863, 0.4902],\n",
       "         [0.7255, 0.7255, 0.7255,  ..., 0.4824, 0.4863, 0.4824],\n",
       "         ...,\n",
       "         [0.3529, 0.3569, 0.3451,  ..., 0.1961, 0.2000, 0.1961],\n",
       "         [0.3294, 0.3373, 0.3333,  ..., 0.1843, 0.2000, 0.1961],\n",
       "         [0.3412, 0.3294, 0.3333,  ..., 0.1765, 0.2000, 0.1961]],\n",
       "\n",
       "        [[0.8118, 0.8078, 0.8039,  ..., 0.6000, 0.6039, 0.6078],\n",
       "         [0.8118, 0.8078, 0.8039,  ..., 0.6000, 0.6039, 0.6039],\n",
       "         [0.8078, 0.8078, 0.8078,  ..., 0.6000, 0.6039, 0.6039],\n",
       "         ...,\n",
       "         [0.5922, 0.5882, 0.5843,  ..., 0.5765, 0.5804, 0.5804],\n",
       "         [0.5804, 0.5843, 0.5804,  ..., 0.5765, 0.5765, 0.5765],\n",
       "         [0.5922, 0.5843, 0.5882,  ..., 0.5765, 0.5725, 0.5725]],\n",
       "\n",
       "        [[0.9294, 0.9216, 0.9216,  ..., 0.8118, 0.8118, 0.8118],\n",
       "         [0.9294, 0.9216, 0.9176,  ..., 0.8118, 0.8157, 0.8118],\n",
       "         [0.9216, 0.9216, 0.9216,  ..., 0.8118, 0.8157, 0.8118],\n",
       "         ...,\n",
       "         [0.7294, 0.7294, 0.7255,  ..., 0.7725, 0.7725, 0.7725],\n",
       "         [0.7216, 0.7255, 0.7216,  ..., 0.7686, 0.7725, 0.7686],\n",
       "         [0.7294, 0.7255, 0.7255,  ..., 0.7686, 0.7686, 0.7647]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_image('images/architecture.jpg',shape=(400,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "torch.Size([3, 800, 1199])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7255, 0.7294, 0.7255,  ..., 0.5020, 0.4941, 0.4941],\n",
       "         [0.7294, 0.7294, 0.7255,  ..., 0.4941, 0.4902, 0.4941],\n",
       "         [0.7294, 0.7294, 0.7255,  ..., 0.4941, 0.4941, 0.4941],\n",
       "         ...,\n",
       "         [0.3373, 0.3373, 0.3216,  ..., 0.2000, 0.1961, 0.1882],\n",
       "         [0.3412, 0.3451, 0.3333,  ..., 0.2118, 0.2000, 0.1804],\n",
       "         [0.3490, 0.3451, 0.3451,  ..., 0.2118, 0.1961, 0.1725]],\n",
       "\n",
       "        [[0.8078, 0.8118, 0.8078,  ..., 0.6118, 0.6078, 0.6078],\n",
       "         [0.8118, 0.8118, 0.8078,  ..., 0.6039, 0.6039, 0.6118],\n",
       "         [0.8118, 0.8118, 0.8078,  ..., 0.6039, 0.6000, 0.6039],\n",
       "         ...,\n",
       "         [0.5882, 0.5843, 0.5765,  ..., 0.5725, 0.5725, 0.5804],\n",
       "         [0.5882, 0.5922, 0.5843,  ..., 0.5765, 0.5765, 0.5765],\n",
       "         [0.5961, 0.5922, 0.5961,  ..., 0.5725, 0.5725, 0.5686]],\n",
       "\n",
       "        [[0.9294, 0.9333, 0.9294,  ..., 0.8157, 0.8118, 0.8118],\n",
       "         [0.9333, 0.9333, 0.9294,  ..., 0.8118, 0.8078, 0.8157],\n",
       "         [0.9333, 0.9333, 0.9294,  ..., 0.8157, 0.8118, 0.8118],\n",
       "         ...,\n",
       "         [0.7255, 0.7255, 0.7176,  ..., 0.7686, 0.7608, 0.7686],\n",
       "         [0.7255, 0.7294, 0.7255,  ..., 0.7686, 0.7647, 0.7647],\n",
       "         [0.7333, 0.7294, 0.7333,  ..., 0.7686, 0.7686, 0.7608]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_image('images/architecture.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
